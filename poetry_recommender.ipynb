{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hashi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The list of packages and modules used\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from surprise import NormalPredictor\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download the list of English words as provided by nltk\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to load the data, which came from the dataset's Github page\n",
    "def load_data(file_name, head = 500):\n",
    "    count = 0\n",
    "    data = []\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for l in fin:\n",
    "            d = json.loads(l)\n",
    "            count += 1\n",
    "            data.append(d)\n",
    "            \n",
    "            if (head is not None) and (count > head):\n",
    "                break\n",
    "    return data\n",
    "\n",
    "# Set the path for where the zipped files are stored\n",
    "path = \"Datasets\"\n",
    "\n",
    "# Load the poetry data, as outlined in the Github\n",
    "# poetry_data = load_data(path + \"\\goodreads_books_poetry.json.gz\")\n",
    "# reviews_data = load_data(path + \"\\goodreads_reviews_poetry.json.gz\")\n",
    "# interactions_data = load_data(path + \"\\goodreads_interactions_poetry.json.gz\")\n",
    "\n",
    "# To print sample records\n",
    "# print(\" == sample record (poetry book) ==\")\n",
    "# print(np.random.choice(poetry_data))\n",
    "# print(\" == sample record (reviews) ==\")\n",
    "# print(np.random.choice(reviews_data))\n",
    "# print(\" == sample record (interactions) ==\")\n",
    "# print(np.random.choice(interactions_data))\n",
    "# print(poetry.sample(n=1))\n",
    "# print(reviews.sample(n=1))\n",
    "# print(interactions.sample(n=1))\n",
    "\n",
    "# Define the maximum amount of data to load, used when programming\n",
    "maximum = 4000\n",
    "\n",
    "# Load the poetry data with pandas\n",
    "poetry_original = pd.read_json(path + \"\\goodreads_books_poetry.json.gz\", lines=True, nrows=maximum)\n",
    "#poetry_headers = list(poetry.columns.values)\n",
    "poetry = poetry_original[[\"country_code\", \"language_code\", \"popular_shelves\", \"average_rating\", \"description\", \"authors\", \"image_url\", \"book_id\", \"ratings_count\", \"title\"]]\n",
    "poetry_old = poetry_original[[\"country_code\", \"language_code\", \"popular_shelves\", \"average_rating\", \"description\", \"authors\", \"image_url\", \"book_id\", \"ratings_count\", \"title\"]]\n",
    "\n",
    "# Load the review data with pandas\n",
    "reviews_original = pd.read_json(path + \"\\goodreads_reviews_poetry.json.gz\", lines=True, nrows=maximum)\n",
    "#reviews_headers = list(reviews_original.columns.values)\n",
    "reviews = reviews_original[[\"user_id\", \"book_id\", \"review_id\", \"rating\", \"review_text\", \"date_added\"]]\n",
    "\n",
    "# Load the interactions data with pandas (larger file)\n",
    "# interactions_original = pd.read_json(path + \"\\goodreads_interactions_poetry.json.gz\", lines=True, chunksize=100000)\n",
    "# pd.DataFrame.from_dict(interactions_data, chunksize=100)\n",
    "interactions_concat = pd.read_json(path + \"\\goodreads_interactions_poetry.json.gz\", lines=True, nrows=maximum)\n",
    "# Store the first 100000 interactions\n",
    "# interactions_concat = pd.concat(interactions_original)\n",
    "# Add all the interactions to an array\n",
    "#interactions_concat = pd.concat(interactions_original, ignore_index=True)\n",
    "interactions_true = interactions_concat.loc[interactions_concat[\"is_read\"] == True]\n",
    "interactions = interactions_true[[\"user_id\", \"book_id\", \"review_id\", \"rating\", \"date_added\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User table\n",
    "# Merge to create a user-interaction table, ensuirng we keep all the matched and unmatched rows, but no repeats of data\n",
    "users = pd.merge(interactions, reviews, how=\"outer\")\n",
    "\n",
    "# Create the contextual data for the users table, including the month and the decade of reviews\n",
    "# users[\"day\"] = users[\"date_added\"].apply(lambda x: 1 if 6<int(x.strftime(\"%H\"))<20 else 0)\n",
    "# users = users.drop(\"date_added\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the books-features vector space model\n",
    "\n",
    "# To create a list for the unique features within a column\n",
    "def unique_features(feature, column_name):\n",
    "    # Go through the column features for each item in the table\n",
    "    for i in poetry[column_name]:\n",
    "        # The feature must be unique and must not be empty\n",
    "        if i not in feature and i != \"\":\n",
    "            # Adds the features to a list\n",
    "            feature.append(i)\n",
    "\n",
    "# To create the products-feature matrices\n",
    "def feature_matrix(feature, column_name):\n",
    "# Creates a column for each feature\n",
    "    for item in feature:\n",
    "        # Add a one to each row where the book shares the feature, otherwise add a zero\n",
    "        poetry[item] = poetry[column_name].apply(lambda x: 1 if item in x else 0)\n",
    "\n",
    "# Create the genre/\"shelves\" part of the book-feature matrix\n",
    "# Single words within the English language\n",
    "words = set(nltk.corpus.words.words())\n",
    "# Irrelevant and obvious tags\n",
    "not_allowed = [\"poem\", \"poet\", \"to-read\"]\n",
    "tags = []\n",
    "# Keep track of the index to later update values\n",
    "index = -1\n",
    "# Search through the popular shelf tags for each poem\n",
    "for i in poetry[\"popular_shelves\"]:\n",
    "    num = 0\n",
    "    index += 1\n",
    "    for j in i:\n",
    "        # Keep the tags that are not within the irrelevant words list\n",
    "        if not any(unncessary in j[\"name\"] for unncessary in not_allowed):\n",
    "            num += 1\n",
    "            # Ensure only the most popular tag is kept if it is not already in the list\n",
    "            if j[\"name\"] in words and num < 3 and j[\"name\"] not in tags:\n",
    "                # Add the words to the list\n",
    "                tags.append(j[\"name\"])\n",
    "            # Update the book-features table cell when there is an instance of a tag\n",
    "            if num < 3:\n",
    "                poetry.at[index, j[\"name\"]] = 1\n",
    "\n",
    "# Creates the genre-feature matrix\n",
    "#index = 0\n",
    "#for i in poetry[\"popular_shelves\"]:\n",
    "#    name = []\n",
    "#    for j in i:\n",
    "#        name.append(j[\"name\"])\n",
    "#    poetry.iloc[index][\"popular_shelves\"] = name\n",
    "#    index += 1\n",
    "# feature_matrix(tags, \"popular_shelves\")\n",
    "#Find the column with this name and change it to 1 in the right place\n",
    "\n",
    "# The unique country of origin array\n",
    "# country = []\n",
    "# Creates the country of origin matrix\n",
    "# unique_features(country, \"country_code\")\n",
    "\n",
    "# The unique book language array\n",
    "# language = []\n",
    "# Create the book language matrix\n",
    "# unique_features(language, \"language_code\")\n",
    "\n",
    "# The unique authors array\n",
    "authors = []\n",
    "# Keep track of the index to update values later\n",
    "index = -1\n",
    "# Iterates through the list of authors\n",
    "for i in poetry[\"authors\"]:\n",
    "    for j in i:\n",
    "        # Creates the unique authors array\n",
    "        if j[\"author_id\"] not in authors:\n",
    "            authors.append(\"author_\" + str(j[\"author_id\"]))\n",
    "        # Update the book-features table cell when there is an instance of an author\n",
    "        poetry.at[index, j[\"author_id\"]] = 1\n",
    "\n",
    "# Creates the authors of the book matrix\n",
    "# feature_matrix(authors, \"authors\")\n",
    "\n",
    "# print(poetry.iloc[:, 10:])\n",
    "# np.count_nonzero(poetry)\n",
    "\n",
    "# Create the keyword analysis matrix, from the poetry description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the user-book matrix by combining the user and books tables\n",
    "\n",
    "# Ensure ratings are placed for the correct user and the book they have rated\n",
    "tmp = users.copy()\n",
    "user_poetry = tmp.pivot_table(index=\"user_id\", columns=\"book_id\", values=\"rating\")\n",
    "\n",
    "# Look up the book_id and replace it with the book name\n",
    "#user_poetry.columns = user_poetry.columns.map(poetry.set_index(\"book_id\")[\"title\"])\n",
    "#user_poetry = user_poetry.loc[:, user_poetry.columns.notna()]\n",
    "# Store the book names in an array\n",
    "#print(len(user_poetry.columns.values))\n",
    "\n",
    "# Ensure the interactions contain books within the books list\n",
    "# for i in users.columns.values:\n",
    "#    if i not in poetry[\"authors\"]:\n",
    "        # Remove all the books that are not within the book list\n",
    "#        users.drop(columns=[i])\n",
    "\n",
    "# Place NaN values where there is an intersection but no rating\n",
    "missing = list(set(user_poetry.index) - set(user_poetry.columns))\n",
    "for value in missing:\n",
    "    user_poetry[value] = np.nan\n",
    "\n",
    "# Perform normalisation as the weighting scheme\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# Scale the data between zero and one\n",
    "user_poetry[:] = min_max_scaler.fit_transform(user_poetry.values)\n",
    "user_poetry = pd.DataFrame(user_poetry[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of the interface - find or create the new user for the system\n",
    "\n",
    "def login():\n",
    "    # Allow the user to enter their username\n",
    "    username = input(\"Enter your username:\")\n",
    "    # Determine whether the user is already in the system\n",
    "    if str(username) in str(user_poetry.index):\n",
    "        # If the user is in the system, set the user index as their username\n",
    "        user_index = int(username)\n",
    "        return user_index\n",
    "    else:\n",
    "        # If the user is not in the system, add them to the user-books matrix\n",
    "        row = len(user_poetry)\n",
    "        # user_poetry.loc[row] = np.NaN\n",
    "        # Store the new user index\n",
    "        user_index = 0\n",
    "        # user_poetry.append(pd.Series(name=user_index))\n",
    "        # Tell the user their username, as they are a new user\n",
    "        print(\"You're a new user, your username is: \" + str(user_poetry.index[0]))\n",
    "        return user_index\n",
    "\n",
    "# print(len(user_poetry.columns.values))\n",
    "# print(len(poetry_old.index))\n",
    "# print(poetry.iloc[:, 10:])\n",
    "# np.count_nonzero(poetry.iloc[:, 10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the content-based recommender system\n",
    "poetry = poetry.drop_duplicates(subset=[\"title\"], keep=\"first\")\n",
    "feature_table = poetry.iloc[:, 10:].fillna(0)\n",
    "\n",
    "# Apply cosine similariity to the new poetry table as we are seeing how similar/different the features are for each film\n",
    "# Use the cosine similarity metric to match how close a rated item (from the user profile vector) is to an unrated item\n",
    "cos_sim = pd.DataFrame(cosine_similarity(feature_table))\n",
    "# print(cos_sim)\n",
    "# np.count_nonzero(cos_sim)\n",
    "\n",
    "# Input the name --> look up the id --> find the index of the id\n",
    "\n",
    "# The function for the content recommender system\n",
    "def content_recommender(search, user_index):\n",
    "    # When the user input is found to be a poem in the system\n",
    "    if str(search) in poetry_old[\"title\"].values:\n",
    "        # Lookup the index for the input\n",
    "        search_index = poetry.index[poetry[\"title\"] == search].tolist()\n",
    "        print(search + \": \" + str(poetry_old.iloc[search_index][\"description\"]))\n",
    "\n",
    "        rating = input(\"Would you like to rate this item? Please type 'y' or 'n'\")\n",
    "        if rating == \"y\":\n",
    "            rating_continued = input(\"What is your rating out of 5?\")\n",
    "            # Update the user rating in the table\n",
    "            # for i in user_index:\n",
    "            #    user_poetry.at[i, search_index] = float(rating_continued/5)\n",
    "\n",
    "        # Find the similar books to the input\n",
    "        similar = list(enumerate(cos_sim[search_index]))\n",
    "        # Sort them based on the most similar ones first\n",
    "        sorted_similar = sorted(similar, key=lambda x:x[1], reverse=True)\n",
    "        # Notify the user if no simialr books were found\n",
    "        if similar == []:\n",
    "            print(\"No similar books were found.\")\n",
    "        if similar != []:\n",
    "            # Print the first few (up to five) similar books based on the search\n",
    "            for i in sorted_similar[:4]:\n",
    "                # print(sorted_similar[:4], i[1])\n",
    "                #output_index = poetry.index[poetry[\"book_id\"] == i[1]].tolist()\n",
    "                #print(output_index)\n",
    "                print(\"Similar books include: \")\n",
    "                print(poetry_old.iloc[i[1]][\"title\"])\n",
    "    # Output when the search does not find a result\n",
    "    else:\n",
    "        print(\"The book is not in the system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-based evalation (personalised vs non-personalised)\n",
    "\n",
    "# Split the user-books table into a test set and a training set\n",
    "# print(user_poetry)\n",
    "\n",
    "# 80% of the data goes to the training set, 20% goes to the test set\n",
    "train, test = train_test_split(user_poetry, test_size=0.2)\n",
    "\n",
    "# print(train)\n",
    "\n",
    "i = 1\n",
    "content_train = train.iloc[i].to_frame(name=\"y\")\n",
    "content_test = test.iloc[i].to_frame(name=\"y\")\n",
    "\n",
    "# Keep the matching book ids in the two tables\n",
    "# for i in train.columns.values:\n",
    "#    if i not in user_poetry.columns.values:\n",
    "#        train.drop(columns=[i])\n",
    "\n",
    "# for i in user_poetry.columns.values:\n",
    "#    if i not in train.columns.values:\n",
    "#        user_poetry.drop(columns=[i])\n",
    "\n",
    "# remove = list(set(user_poetry.columns.values).symmetric_difference(train.columns.values))\n",
    "# print(remove)\n",
    "\n",
    "# usr_ft = tf.matmul(usr, prd)\n",
    "# weights = usr_ft / tf.reduce_sum(usr_ft, axis=1, keepdims=True)\n",
    "# pred = tf.matmul(weights, prd.T)\n",
    "\n",
    "# user_poetry = user_poetry.fillna(0)\n",
    "# similarity_matrix = linear_kernel(user_poetry, user_poetry)\n",
    "# print(similarity_matrix)\n",
    "\n",
    "# mapping = pd.Series(poetry_old.index, index = poetry_old[\"title\"])\n",
    "# title = mapping[search]\n",
    "\n",
    "# Use the k-nearest neighbour method to compare unrated items to all stored items and assign neighbour classes\n",
    "# similarity = list(enumerate(user_poetry[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative-based (model-based) - using other user information\n",
    "# Transpose the matrix to perform a dot product\n",
    "transposed = user_poetry.T.fillna(0)\n",
    "\n",
    "# Decompose the matrix with SVD\n",
    "SVD = TruncatedSVD(n_components=12, random_state=17)\n",
    "# Fit the matrix to the model\n",
    "matrix = SVD.fit_transform(transposed)\n",
    "\n",
    "# param_grid = {'n_factors':[50,100,150],'n_epochs':[20,30],  'lr_all':[0.005,0.01],'reg_all':[0.02,0.1]}\n",
    "# gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "# gs.fit(transposed)\n",
    "# print(gs.best_score['rmse'])\n",
    "# print(gs.best_score['mae'])\n",
    "# params = gs.best_params['rmse']\n",
    "# print(params)\n",
    "\n",
    "# Create a correlation matrix between all the user book ratings\n",
    "correlation = np.corrcoef(matrix)\n",
    "\n",
    "# To know the mappings between the ids and the index values\n",
    "book_ids = list(user_poetry.columns)\n",
    "\n",
    "# The function for the collaborative recommender system\n",
    "def collaborative_recommender(search_collab, user_index):\n",
    "    # When the user input is found to be a poem in the system\n",
    "    if str(search_collab) in poetry[\"title\"].values:\n",
    "        # Lookup the index for the input\n",
    "        search_index = poetry.index[poetry[\"title\"] == search_collab].tolist()\n",
    "\n",
    "        rating = input(\"Would you like to rate this item? Please type 'y' or 'n'\")\n",
    "        if rating == \"y\":\n",
    "            rating_continued = input(\"What is your rating out of 5?\")\n",
    "            # Update the user rating in the table\n",
    "            # for i in user_index:\n",
    "            #    user_poetry.at[i, search_index] = float(rating_continued/5)\n",
    "\n",
    "        # Display the first five items (maximum), that are most similar to the input\n",
    "        for i in search_index:\n",
    "            similar = list(poetry_old.columns[(correlation[i]<1.0) & (correlation[i]>0.5)])\n",
    "            print(search_collab + \": \" + str(poetry_old.iloc[i][\"description\"]))\n",
    "        # Notify the user if no similar books were found\n",
    "        if similar == []:\n",
    "            print(\"No similar books were found.\")\n",
    "        if similar != []:\n",
    "            for i in similar[:4]:\n",
    "                # We get ids instead, look-up the index in the book_ids array\n",
    "                position = book_ids.index(i)\n",
    "                print(\"Similar books include: \")\n",
    "                print(poetry_old.iloc[position][\"title\"])\n",
    "        # Output when the search does not find a result\n",
    "    else:\n",
    "        print(\"The book is not in the system or no similar books were found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative-based evaluation (personalised vs non-personalised)\n",
    "\n",
    "# Perform parameter tuning\n",
    "# transposed = user_poetry.T\n",
    "\n",
    "# Create a correlation matrix between all the books\n",
    "# correlation = np.corrcoef(matrix)\n",
    "# book_ids = list(user_poetry.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-personalised functions and calculations\n",
    "# Store the most popular books on the system\n",
    "popular_books = []\n",
    "# Keep track of the item index\n",
    "index = -1\n",
    "for i in poetry_old[\"popular_shelves\"]:\n",
    "    index += 1\n",
    "    for j in i:\n",
    "        # Store all the books with a count over 100 and those marked as \"to-read\"\n",
    "        if int(j[\"count\"]) > 100 and str(j[\"name\"]) == \"to-read\":\n",
    "            popular_books.append(poetry_old.iloc[index][\"title\"])\n",
    "\n",
    "def popular_books_recommendation():\n",
    "    # Print a random five books from the most popular books list\n",
    "    print(\"Popular books:\")\n",
    "    for i in range(5):\n",
    "        print(random.choice(popular_books), end = \"\\n\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When using this system, contextual data and data based on rating you make will be collected. Please continue if you consent.\n",
      "You're a new user, your username is: 004d5e96c8a318aeb006af50f8cc949c\n",
      "Into Temptation: 6    Into Temptation is the debut collection of poe...\n",
      "Name: description, dtype: object\n",
      "Similar books include: \n",
      "Into Temptation\n"
     ]
    }
   ],
   "source": [
    "# User interface\n",
    "# Allow the user to choose between the non-personalised and personalised recommender systems\n",
    "print(\"When using this system, contextual data and data based on rating you make will be collected. Please continue if you consent.\")\n",
    "start = input(\"This is a poetry collection hybrid recommender system, using content and model-based collaborative filtering. \\nPlease type 'np' to use the non-personalised recommender system or 'p' to use the personalised recommender system.\")\n",
    "\n",
    "# Non-personalised recommender\n",
    "if start == \"np\":\n",
    "    refresh = input(\"Type in 'n' to see new recommendations, or 'q' to quit the system.\")\n",
    "    # Loop until the user quits\n",
    "    while refresh != \"q\":\n",
    "        # Provide the most popular books\n",
    "        popular_books_recommendation()\n",
    "        # Allow the user to see new recommendations or quit the system\n",
    "        refresh = input(\"Type in 'n' to see new recommendations, or 'q' to quit the system.\")\n",
    "    \n",
    "    # High average ratings\n",
    "    \n",
    "# Personalised recommender\n",
    "if start == \"p\":\n",
    "    # Allow the user to login to add ratings\n",
    "    user_index = login()\n",
    "    # Allow the user to search books, see new recommendations or quit the system\n",
    "    action = input(\"Type in 's' to search a book title (to learn more about it, find similar books and rate it) or 'q' to quit the system.\")\n",
    "    iteration = 0\n",
    "    # Loop until the user quits\n",
    "    while action != \"q\":\n",
    "        # Store the number of ratings the user has made\n",
    "        number_of_ratings = user_poetry.iloc[user_index].sum()\n",
    "        iteration += 1\n",
    "        # Hybrid approach\n",
    "        # Use switching and change recommender when there is enough interaction data\n",
    "        # Primarly use content-based filtering to begin with, to avoid the cold-start problem and to deal with the spare data\n",
    "        if (action == \"s\" and number_of_ratings < 20):\n",
    "            # Use the content-based recommender when there are few user ratings\n",
    "            content_recommender(input(\"Enter the name of a poetry collection to find similar titles.\"), user_index)\n",
    "            action = input(\"Type in 's' to search a book title (to learn more about it, find similar books and rate it) or 'q' to quit the system.\")\n",
    "        if action == \"s\" and number_of_ratings > 20 or iteration == 2:        \n",
    "            # Use the collaborative-based recommender when there are enough user ratings or when the system has been used long enough\n",
    "            collaborative_recommender(input(\"Enter the name of a poetry collection to find similar titles.\"), user_index)\n",
    "            action = input(\"Type in 's' to search a book title (to learn more about it, find similar books and rate it) or 'q' to quit the system.\")\n",
    "\n",
    "else:\n",
    "    # If anything else is typed in, notify the user to restart the system\n",
    "    print(\"\\nPlease restart the system.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
